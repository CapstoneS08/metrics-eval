{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6862c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\n",
      "INPUT_JSONL: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\data\\issue_tracking\\val\\claude_48.jsonl\n",
      "INPUT exists: True\n",
      "OUT_DIR: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\n",
      "EXPERT_DIR: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\\expert_val\n"
     ]
    }
   ],
   "source": [
    "# Reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Project root (folder that contains \"data/\")\n",
    "PROJECT_ROOT = Path(r\"C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\")\n",
    "\n",
    "# Input: issue tracking WhatsApp-style synthetic val set\n",
    "INPUT_JSONL = PROJECT_ROOT / \"data\" / \"issue_tracking\" / \"val\" / \"claude_48.jsonl\"\n",
    "\n",
    "# Output root\n",
    "OUT_DIR = PROJECT_ROOT / \"models\" / \"issue_tracking\" / \"results\"\n",
    "EXPERT_DIR = OUT_DIR / \"expert_val\"\n",
    "EXPERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"INPUT_JSONL:\", INPUT_JSONL)\n",
    "print(\"INPUT exists:\", INPUT_JSONL.exists())\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n",
    "print(\"EXPERT_DIR:\", EXPERT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4717addc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env: C:\\Users\\tengc\\Downloads\\metrics-eval\\.env\n",
      "OPENAI_API_KEY found: True\n",
      "ANTHROPIC_API_KEY found: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_PATH = PROJECT_ROOT.parent / \".env\"  # you said: Loaded .env: C:\\Users\\tengc\\Downloads\\metrics-eval\\.env\n",
    "\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(dotenv_path=ENV_PATH)\n",
    "    print(f\"Loaded .env: {ENV_PATH}\")\n",
    "else:\n",
    "    print(\"WARNING: .env not found at:\", ENV_PATH)\n",
    "\n",
    "print(\"OPENAI_API_KEY found:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
    "print(\"ANTHROPIC_API_KEY found:\", bool(os.environ.get(\"ANTHROPIC_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20cc5ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env: C:\\Users\\tengc\\Downloads\\metrics-eval\\.env\n",
      "OPENAI_API_KEY found: True\n",
      "ANTHROPIC_API_KEY found: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_PATH = PROJECT_ROOT.parent / \".env\"  # you said: Loaded .env: C:\\Users\\tengc\\Downloads\\metrics-eval\\.env\n",
    "\n",
    "if ENV_PATH.exists():\n",
    "    load_dotenv(dotenv_path=ENV_PATH)\n",
    "    print(f\"Loaded .env: {ENV_PATH}\")\n",
    "else:\n",
    "    print(\"WARNING: .env not found at:\", ENV_PATH)\n",
    "\n",
    "print(\"OPENAI_API_KEY found:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n",
    "print(\"ANTHROPIC_API_KEY found:\", bool(os.environ.get(\"ANTHROPIC_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c104a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported pipeline from: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\scripts\\pipeline.py\n"
     ]
    }
   ],
   "source": [
    "SCRIPTS_DIR = PROJECT_ROOT / \"models\" / \"issue_tracking\" / \"scripts\"\n",
    "sys.path.append(str(SCRIPTS_DIR))\n",
    "\n",
    "import pipeline\n",
    "from export_issue_tracking_expert_sheet import export_workbook as export_issue_workbook\n",
    "\n",
    "print(\"Imported pipeline from:\", pipeline.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8517aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODELS: ['gpt-5', 'claude-sonnet-4-5']\n",
      "TEST_MODE: False\n",
      "MAX_ROWS: 30\n"
     ]
    }
   ],
   "source": [
    "GPT_MODEL = \"gpt-5\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-5\"\n",
    "\n",
    "MODELS = [GPT_MODEL, CLAUDE_MODEL]\n",
    "\n",
    "TEST_MODE = False        # set True for quick test\n",
    "MAX_ROWS = 30            # only used if TEST_MODE=True\n",
    "SLEEP_SEC = 0.0          # set small sleep if rate limits\n",
    "\n",
    "print(\"MODELS:\", MODELS)\n",
    "print(\"TEST_MODE:\", TEST_MODE)\n",
    "print(\"MAX_ROWS:\", MAX_ROWS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b98f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Running model: gpt-5\n",
      "Saving to: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\\gpt-5\n",
      "Reading input from: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\data\\issue_tracking\\val\\claude_48.jsonl\n",
      "[1/48] Processing message_id=WAM_001 ...\n",
      "[2/48] Processing message_id=WAM_003 ...\n",
      "[3/48] Processing message_id=WAM_005 ...\n",
      "[4/48] Processing message_id=WAM_006 ...\n",
      "[5/48] Processing message_id=WAM_008 ...\n",
      "[6/48] Processing message_id=WAM_010 ...\n",
      "[7/48] Processing message_id=WAM_011 ...\n",
      "[8/48] Processing message_id=WAM_013 ...\n",
      "[9/48] Processing message_id=WAM_015 ...\n",
      "[10/48] Processing message_id=WAM_017 ...\n",
      "[11/48] Processing message_id=WAM_018 ...\n",
      "[12/48] Processing message_id=WAM_020 ...\n",
      "[13/48] Processing message_id=WAM_022 ...\n",
      "[14/48] Processing message_id=WAM_024 ...\n",
      "[15/48] Processing message_id=WAM_026 ...\n",
      "[16/48] Processing message_id=WAM_028 ...\n",
      "[17/48] Processing message_id=WAM_029 ...\n",
      "[18/48] Processing message_id=WAM_031 ...\n",
      "[19/48] Processing message_id=WAM_033 ...\n",
      "[20/48] Processing message_id=WAM_034 ...\n",
      "[21/48] Processing message_id=WAM_036 ...\n",
      "[22/48] Processing message_id=WAM_038 ...\n",
      "[23/48] Processing message_id=WAM_040 ...\n",
      "[24/48] Processing message_id=WAM_042 ...\n",
      "[25/48] Processing message_id=WAM_044 ...\n",
      "[26/48] Processing message_id=WAM_046 ...\n",
      "[27/48] Processing message_id=WAM_048 ...\n",
      "[28/48] Processing message_id=WAM_050 ...\n",
      "[29/48] Processing message_id=S_001 ...\n",
      "[30/48] Processing message_id=S_002 ...\n",
      "[31/48] Processing message_id=S_003 ...\n",
      "[32/48] Processing message_id=S_004 ...\n",
      "[33/48] Processing message_id=S_005 ...\n",
      "[34/48] Processing message_id=S_006 ...\n",
      "[35/48] Processing message_id=S_007 ...\n",
      "[36/48] Processing message_id=S_008 ...\n",
      "[37/48] Processing message_id=S_009 ...\n",
      "[38/48] Processing message_id=S_010 ...\n",
      "[39/48] Processing message_id=S_011 ...\n",
      "[40/48] Processing message_id=S_012 ...\n",
      "[41/48] Processing message_id=S_013 ...\n",
      "[42/48] Processing message_id=S_014 ...\n",
      "[43/48] Processing message_id=S_015 ...\n",
      "[44/48] Processing message_id=S_016 ...\n",
      "[45/48] Processing message_id=S_017 ...\n",
      "[46/48] Processing message_id=S_018 ...\n",
      "[47/48] Processing message_id=S_019 ...\n",
      "[48/48] Processing message_id=S_020 ...\n",
      "Writing JSONL to: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\\gpt-5\\issue_tracking_results_gpt_5.jsonl\n",
      "Writing CSV to: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\\gpt-5\\issue_tracking_results_gpt_5.csv\n",
      "Done.\n",
      "\n",
      "========================================\n",
      "Running model: claude-sonnet-4-5\n",
      "Saving to: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\models\\issue_tracking\\results\\claude-sonnet-4-5\n",
      "Reading input from: C:\\Users\\tengc\\Downloads\\metrics-eval\\metrics_eval\\data\\issue_tracking\\val\\claude_48.jsonl\n",
      "[1/48] Processing message_id=WAM_001 ...\n",
      "[2/48] Processing message_id=WAM_003 ...\n",
      "[3/48] Processing message_id=WAM_005 ...\n",
      "[4/48] Processing message_id=WAM_006 ...\n",
      "[5/48] Processing message_id=WAM_008 ...\n",
      "[6/48] Processing message_id=WAM_010 ...\n",
      "[7/48] Processing message_id=WAM_011 ...\n",
      "[8/48] Processing message_id=WAM_013 ...\n",
      "[9/48] Processing message_id=WAM_015 ...\n",
      "[10/48] Processing message_id=WAM_017 ...\n",
      "[11/48] Processing message_id=WAM_018 ...\n",
      "[12/48] Processing message_id=WAM_020 ...\n",
      "[13/48] Processing message_id=WAM_022 ...\n",
      "[14/48] Processing message_id=WAM_024 ...\n",
      "[15/48] Processing message_id=WAM_026 ...\n",
      "[16/48] Processing message_id=WAM_028 ...\n",
      "[17/48] Processing message_id=WAM_029 ...\n",
      "[18/48] Processing message_id=WAM_031 ...\n",
      "[19/48] Processing message_id=WAM_033 ...\n",
      "[20/48] Processing message_id=WAM_034 ...\n",
      "[21/48] Processing message_id=WAM_036 ...\n",
      "[22/48] Processing message_id=WAM_038 ...\n",
      "[23/48] Processing message_id=WAM_040 ...\n",
      "[24/48] Processing message_id=WAM_042 ...\n",
      "[25/48] Processing message_id=WAM_044 ...\n",
      "[26/48] Processing message_id=WAM_046 ...\n",
      "[27/48] Processing message_id=WAM_048 ...\n",
      "[28/48] Processing message_id=WAM_050 ...\n",
      "[29/48] Processing message_id=S_001 ...\n",
      "[30/48] Processing message_id=S_002 ...\n",
      "[31/48] Processing message_id=S_003 ...\n",
      "[32/48] Processing message_id=S_004 ...\n",
      "[33/48] Processing message_id=S_005 ...\n",
      "[34/48] Processing message_id=S_006 ...\n",
      "[35/48] Processing message_id=S_007 ...\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "# reload in case you edited scripts\n",
    "reload(pipeline)\n",
    "\n",
    "model_dfs = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    model_out_dir = OUT_DIR / model_name.replace(\"/\", \"_\")\n",
    "    model_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(\"\\n========================================\")\n",
    "    print(\"Running model:\", model_name)\n",
    "    print(\"Saving to:\", model_out_dir)\n",
    "\n",
    "    df_out = pipeline.run_issue_tracking_pipeline(\n",
    "        input_path=INPUT_JSONL,\n",
    "        output_dir=model_out_dir,\n",
    "        model=model_name,\n",
    "        test_mode=TEST_MODE,\n",
    "        max_rows=MAX_ROWS,\n",
    "        sleep_sec=SLEEP_SEC,\n",
    "    )\n",
    "    model_dfs[model_name] = df_out\n",
    "\n",
    "print(\"\\nDone. Example rows:\")\n",
    "model_dfs[GPT_MODEL].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_nonexpert_metrics(df: pd.DataFrame) -> dict:\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return {\"n_rows\": 0}\n",
    "\n",
    "    schema_ok_pct = round(float(df[\"schema_ok\"].mean()) * 100, 2) if \"schema_ok\" in df.columns else 0.0\n",
    "\n",
    "    # error is \"\" if ok\n",
    "    err = df[\"error\"].fillna(\"\").astype(str).str.strip()\n",
    "    error_rate_pct = round(float((err != \"\").mean()) * 100, 2)\n",
    "\n",
    "    # basic field emptiness (sanity)\n",
    "    issue_empty_pct = round(float((df[\"issue\"].fillna(\"\").astype(str).str.strip() == \"\").mean()) * 100, 2)\n",
    "    assigned_empty_pct = round(float((df[\"assigned_to\"].fillna(\"\").astype(str).str.strip() == \"\").mean()) * 100, 2)\n",
    "\n",
    "    comments_len = df[\"comments\"].fillna(\"\").astype(str).str.len()\n",
    "    avg_comments_len = round(float(comments_len.mean()), 2)\n",
    "    median_comments_len = float(comments_len.median())\n",
    "\n",
    "    return {\n",
    "        \"n_rows\": int(n),\n",
    "        \"schema_ok_pct\": schema_ok_pct,\n",
    "        \"error_rate_pct\": error_rate_pct,\n",
    "        \"issue_empty_pct\": issue_empty_pct,\n",
    "        \"assigned_to_empty_pct\": assigned_empty_pct,\n",
    "        \"avg_comments_len_chars\": avg_comments_len,\n",
    "        \"median_comments_len_chars\": median_comments_len,\n",
    "    }\n",
    "\n",
    "metrics_rows = []\n",
    "for m in MODELS:\n",
    "    met = compute_nonexpert_metrics(dfs[m])\n",
    "    met[\"model\"] = m\n",
    "    metrics_rows.append(met)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows).sort_values(\"model\")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a6ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Schema OK bar chart\n",
    "plt.figure()\n",
    "plt.bar(metrics_df[\"model\"], metrics_df[\"schema_ok_pct\"])\n",
    "plt.xticks(rotation=20, ha=\"right\")\n",
    "plt.ylabel(\"Schema OK (%)\")\n",
    "plt.title(\"Issue Tracking: Schema OK Rate by Model\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Issue category distribution (stacked-ish shown as separate bar charts)\n",
    "for m in MODELS:\n",
    "    df = dfs[m].copy()\n",
    "    counts = df[\"issue\"].fillna(\"\").astype(str).value_counts()\n",
    "    plt.figure()\n",
    "    plt.bar(counts.index.astype(str), counts.values)\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Issue category distribution - {m}\")\n",
    "    plt.show()\n",
    "\n",
    "# 3) Comments length histogram\n",
    "for m in MODELS:\n",
    "    lengths = dfs[m][\"comments\"].fillna(\"\").astype(str).str.len()\n",
    "    plt.figure()\n",
    "    plt.hist(lengths, bins=30)\n",
    "    plt.xlabel(\"Comments length (chars)\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Comments length distribution - {m}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47c819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose sample size per model for the client to validate\n",
    "SAMPLE_N = 30\n",
    "\n",
    "out_xlsx = EXPERT_DIR / \"issue_tracking_expert_validation.xlsx\"\n",
    "\n",
    "export_issue_workbook(\n",
    "    gpt_df=dfs[GPT_MODEL],\n",
    "    claude_df=dfs[CLAUDE_MODEL],\n",
    "    out_path=out_xlsx,\n",
    "    sample_n=SAMPLE_N,\n",
    ")\n",
    "\n",
    "print(\"âœ… Exported:\", out_xlsx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcfe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_csv = OUT_DIR / \"nonexpert_metrics_issue_tracking.csv\"\n",
    "metrics_df.to_csv(metrics_csv, index=False)\n",
    "print(\"Saved:\", metrics_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b368a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
